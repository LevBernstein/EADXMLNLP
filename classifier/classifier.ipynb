{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['access', 'accession', 'accretion', 'accrual', 'acquisition']\n",
      "['work', 'working files', 'workstation', 'writ', 'write']\n"
     ]
    }
   ],
   "source": [
    "termsFile = open(\"ProcessTerms.csv\",\"r\")\n",
    "termsList = [x[0] for x in list(csv.reader(termsFile, delimiter=\",\"))]\n",
    "termsFile.close()\n",
    "\n",
    "print(termsList[:5])\n",
    "print(termsList[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the henriette d. avram marc development collection contains items originating primarily during the period from 1966 to 1976 although there are some materials from the early 1960s, late 1970s, and early 1980s. the contents of the collection are described below in the context of the major accomplishments of the program and the resulting publications. the following essay documents the provenance and organization of the collection, creation and development of the marc standard, and automation activities in the library of congress. it is reproduced from lenore maruyama's \"the marc archives: a register of records in the library of congress\" (may 8, 1987, 74 pp.), which is available in the manuscript reading room. it has been revised to conform to current standards for finding aids in the manuscript division. office files covering the marc pilot project and the activities of the marc development office through the early 1970s. these items had been placed in storage with the central services division. office files from the marc development office, which had been disbanded in 1977. functions had been divided among the automated systems office, the network development and marc standards office, and the automation planning and liaison office. these items had been transferred to the automated systems office and had been placed in storage with the central services division. some of the office files from the office of the information systems specialist and most of the files from the information systems office from about 1964-1977, with some items from the automated systems office dating from the early 1980s, have also been placed in storage with the central services division. although there are some items related to the marc pilot project, the marc system, and the marc development office, most of these files deal with the automation efforts taking place before marc or outside the scope of marc. office files from processing services department that contained materials about automation of technical processing, activities and projects of the marc development office (which was a unit under processing services), and activities of ansi z39. these items had been placed in storage with the central services division. office files containing materials about automation at the library of congress around the late 1960s and the early 1970s from the former assistant librarian of congress, elizabeth hamer kegan. these items had been placed in storage with the central services division. office files containing materials about projects like the comarc pilot project or the marc formats (including the bibliographic codes and character sets and the liaison with users) that had been the responsibility of the marc development office. in addition, materials from more recent efforts, such as the network advisory committee, network technical architecture group, linked systems project, national level bibliographic record documents, network data base design project, or international marc projects (including the work done by the british in the late 1960s), have been included, as well as numerous publications about or by library network organizations. these items had been obtained directly from the office. office files and publications kept by patricia e. parker and gail l. hitchcock, who as staff members of the marc development office, were responsible for maintaining the editing guides and internal specification documents and a collection of items published by staff of the marc development office. ms. parker was also involved with the standardization work for bibliographic codes. these items were transferred directly from these individuals. other miscellaneous publications had been collected by other staff members. office files from kay d. guiles containing items about the international meeting(s) of cataloging experts, the international standard bibliographic description, miscellaneous items about the recon pilot project and the early days of the marc distribution service, and copies of correspondence from 1968-1976. as the senior bibliographic specialist in the marc development office he was responsible for the coordination with the cataloging divisions concerning the impact of the (new) cataloging code on the marc formats and the implementation of the new cataloging rules. these items were transferred directly from mr. guiles' office. personal files from henriette d. avram containing miscellaneous items about marc and recon, an assortment of publications, copies of articles or reprints about her (but not by her), and letters from the participants of the marc pilot project accepting the invitation from the library to participate in the project. in addition, a number of souvenirs or memorabilia have been included. these items were transferred directly from mrs. avram's files. a bibliography of items written by mrs. avram has been updated through 1988 and is included in the archives. copies of annual reports collected for the archive and miscellaneous publications. office files from the technical process research office and copies of drafts, reprints, and other documents prepared by the staff members of tpro, which was disbanded in 1976. also microfilm copy of official catalog sample, listings, and copies of cards pulled in june 1969. plus a comarc notebook, the iss planning memorandum series, the marc planning paper series, and other miscellaneous publications dealing with an automated technical processing system at the library of congress. ms. biebel was a staff member of tpro and had worked on the comarc project. office files from the office of the information systems office from the early 1960s to about 1967. these files contain materials related to early automation efforts, including the conference on libraries and automation held in 1963, the king survey (of automation at the library of congress), the programming and systems analysis services provided by united aircraft for the marc pilot project and the overall automation program at lc, and the survey conducted by nelson associates for the national serials data program. these items had been placed in storage with the central services division. office and personal files from the late josephine s. pulsifer, who, in addition to being a staff member of the marc development office and the automation planning and liaison office, had worked for the washington state library at the time of the marc pilot project and later for a consultant in the washington area. most of the items consist of background material for the first edition of a marc history and the publications collected for a bibliography of works written by henriette d. avram. miscellaneous reports and documents that have been obtained sporadically to add to the archives. includes several photographs. copies of some items from contract files of the council on library resources, washington, d.c., dealing with projects sponsored by the council that predated and were the precursors of the marc pilot project. although the library of congress had been involved with these projects, the studies were performed by an outside consultant. also included some of the items dealing with the marc pilot project. the records of the henriette d. avram marc development collection were processed in 1987 by lenore s. maruyama, maruyama associates, inc., arlington, va. the finding aid was revised in 1989 and 2017. this collection is arranged in fourteen series: arranged in three subseries: automated systems office/marc development office; office of the information systems specialist; and information systems office\n"
     ]
    }
   ],
   "source": [
    "textFile = open(\"0.txt\",\"r\")\n",
    "textStr = textFile.readlines()[0]\n",
    "textFile.close()\n",
    "\n",
    "print(textStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the henriette d avram marc development collection contains items originating primarily during the period from  to  although there are some materials from the early s late s and early s',\n",
       " 'the contents of the collection are described below in the context of the major accomplishments of the program and the resulting publications',\n",
       " 'the following essay documents the provenance and organization of the collection creation and development of the marc standard and automation activities in the library of congress',\n",
       " 'it is reproduced from lenore maruyamas the marc archives a register of records in the library of congress may    pp which is available in the manuscript reading room',\n",
       " 'it has been revised to conform to current standards for finding aids in the manuscript division']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def removePunctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "def removeNumbers(text):\n",
    "    return re.sub(r'[0-9]', '', text)\n",
    "\n",
    "def removeAll(text):\n",
    "    return removePunctuation(removeNumbers(text))\n",
    "\n",
    "def sentenceSplit(text):\n",
    "    text = text.replace('pp.','pp') # So that \"pp.\" doesn't trigger a sentence split\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    return [removePunctuation(removeNumbers(sentence)) for sentence in tokenizer.tokenize(text)]\n",
    "\n",
    "sentenceSplit(textStr)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'is',\n",
       " 'reproduced',\n",
       " 'from',\n",
       " 'lenore',\n",
       " 'maruyamas',\n",
       " 'the',\n",
       " 'marc',\n",
       " 'archives',\n",
       " 'a',\n",
       " 'register',\n",
       " 'of',\n",
       " 'records',\n",
       " 'in',\n",
       " 'the',\n",
       " 'library',\n",
       " 'of',\n",
       " 'congress',\n",
       " 'may',\n",
       " 'pp',\n",
       " 'which',\n",
       " 'is',\n",
       " 'available',\n",
       " 'in',\n",
       " 'the',\n",
       " 'manuscript',\n",
       " 'reading',\n",
       " 'room']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = sentenceSplit(textStr)[:5]\n",
    "\n",
    "def tokenize(text):\n",
    "    return [x for x in text.split(' ') if x != '']\n",
    "\n",
    "tokenize(sents[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lbl2vec import Lbl2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txtFilepath(num):\n",
    "    return os.getcwd()[:-10]+'txtfiles\\\\'+str(num)+'.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_formatted = []\n",
    "docs_numberless = []\n",
    "\n",
    "for doc in range(919):\n",
    "    file = open(txtFilepath(doc),\"r\")\n",
    "    fileRead = file.read()\n",
    "    docs_numberless.append(removeNumbers(fileRead))\n",
    "    docs_formatted.append(removeAll(fileRead))\n",
    "    file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = Vectorizer.fit_transform(docs_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(919, 48312)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "wordFrequencies=pd.DataFrame(X.toarray(), columns=Vectorizer.get_feature_names()).sort_values(by=0,axis=1,ascending=False)\n",
    "wordFrequencies.columns[:5]\n",
    "wordFrequencies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199 terms removed\n"
     ]
    }
   ],
   "source": [
    "termsRemoved = 0\n",
    "for term in termsList:\n",
    "    try:\n",
    "        wordFrequencies = wordFrequencies.drop([term],axis=1)\n",
    "        termsRemoved += 1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(str(termsRemoved)+\" terms removed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(919, 48113)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordFrequencies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidfTerms(df, doc, amt):\n",
    "    sorted = df.sort_values(by=doc,axis=1,ascending=False)\n",
    "    return list(sorted.columns[:amt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['marc',\n",
       " 'office',\n",
       " 'automation',\n",
       " 'development',\n",
       " 'systems',\n",
       " 'pilot',\n",
       " 'files',\n",
       " 'items',\n",
       " 'project',\n",
       " 'avram']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfTerms(wordFrequencies,0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words='the henriette d avram marc development collection contains items originating primarily during the period from  to  although there are some materials from the early s late s and early s the contents of the collection are described below in the context of the major accomplishments of the program and the resulting publications', tags=[0]),\n",
       " TaggedDocument(words='the following essay documents the provenance and organization of the collection creation and development of the marc standard and automation activities in the library of congress', tags=[1]),\n",
       " TaggedDocument(words='it is reproduced from lenore maruyamas the marc archives a register of records in the library of congress may    pp which is available in the manuscript reading room', tags=[2]),\n",
       " TaggedDocument(words='it has been revised to conform to current standards for finding aids in the manuscript division', tags=[3]),\n",
       " TaggedDocument(words='office files covering the marc pilot project and the activities of the marc development office through the early s these items had been placed in storage with the central services division', tags=[4]),\n",
       " TaggedDocument(words='office files from the marc development office which had been disbanded in ', tags=[5]),\n",
       " TaggedDocument(words='functions had been divided among the automated systems office the network development and marc standards office and the automation planning and liaison office', tags=[6]),\n",
       " TaggedDocument(words='these items had been transferred to the automated systems office and had been placed in storage with the central services division', tags=[7]),\n",
       " TaggedDocument(words='some of the office files from the office of the information systems specialist and most of the files from the information systems office from about  with some items from the automated systems office dating from the early s have also been placed in storage with the central services division', tags=[8]),\n",
       " TaggedDocument(words='although there are some items related to the marc pilot project the marc system and the marc development office most of these files deal with the automation efforts taking place before marc or outside the scope of marc', tags=[9]),\n",
       " TaggedDocument(words='office files from processing services department that contained materials about automation of technical processing activities and projects of the marc development office which was a unit under processing services and activities of ansi z these items had been placed in storage with the central services division', tags=[10]),\n",
       " TaggedDocument(words='office files containing materials about automation at the library of congress around the late s and the early s from the former assistant librarian of congress elizabeth hamer kegan', tags=[11]),\n",
       " TaggedDocument(words='these items had been placed in storage with the central services division', tags=[12]),\n",
       " TaggedDocument(words='office files containing materials about projects like the comarc pilot project or the marc formats including the bibliographic codes and character sets and the liaison with users that had been the responsibility of the marc development office', tags=[13]),\n",
       " TaggedDocument(words='in addition materials from more recent efforts such as the network advisory committee network technical architecture group linked systems project national level bibliographic record documents network data base design project or international marc projects including the work done by the british in the late s have been included as well as numerous publications about or by library network organizations', tags=[14]),\n",
       " TaggedDocument(words='these items had been obtained directly from the office', tags=[15]),\n",
       " TaggedDocument(words='office files and publications kept by patricia e parker and gail l hitchcock who as staff members of the marc development office were responsible for maintaining the editing guides and internal specification documents and a collection of items published by staff of the marc development office', tags=[16]),\n",
       " TaggedDocument(words='ms parker was also involved with the standardization work for bibliographic codes', tags=[17]),\n",
       " TaggedDocument(words='these items were transferred directly from these individuals', tags=[18]),\n",
       " TaggedDocument(words='other miscellaneous publications had been collected by other staff members', tags=[19]),\n",
       " TaggedDocument(words='office files from kay d guiles containing items about the international meetings of cataloging experts the international standard bibliographic description miscellaneous items about the recon pilot project and the early days of the marc distribution service and copies of correspondence from ', tags=[20]),\n",
       " TaggedDocument(words='as the senior bibliographic specialist in the marc development office he was responsible for the coordination with the cataloging divisions concerning the impact of the new cataloging code on the marc formats and the implementation of the new cataloging rules', tags=[21]),\n",
       " TaggedDocument(words='these items were transferred directly from mr guiles office', tags=[22]),\n",
       " TaggedDocument(words='personal files from henriette d avram containing miscellaneous items about marc and recon an assortment of publications copies of articles or reprints about her but not by her and letters from the participants of the marc pilot project accepting the invitation from the library to participate in the project', tags=[23]),\n",
       " TaggedDocument(words='in addition a number of souvenirs or memorabilia have been included', tags=[24]),\n",
       " TaggedDocument(words='these items were transferred directly from mrs avrams files', tags=[25]),\n",
       " TaggedDocument(words='a bibliography of items written by mrs avram has been updated through  and is included in the archives', tags=[26]),\n",
       " TaggedDocument(words='copies of annual reports collected for the archive and miscellaneous publications', tags=[27]),\n",
       " TaggedDocument(words='office files from the technical process research office and copies of drafts reprints and other documents prepared by the staff members of tpro which was disbanded in ', tags=[28]),\n",
       " TaggedDocument(words='also microfilm copy of official catalog sample listings and copies of cards pulled in june ', tags=[29]),\n",
       " TaggedDocument(words='plus a comarc notebook the iss planning memorandum series the marc planning paper series and other miscellaneous publications dealing with an automated technical processing system at the library of congress', tags=[30]),\n",
       " TaggedDocument(words='ms biebel was a staff member of tpro and had worked on the comarc project', tags=[31]),\n",
       " TaggedDocument(words='office files from the office of the information systems office from the early s to about ', tags=[32]),\n",
       " TaggedDocument(words='these files contain materials related to early automation efforts including the conference on libraries and automation held in  the king survey of automation at the library of congress the programming and systems analysis services provided by united aircraft for the marc pilot project and the overall automation program at lc and the survey conducted by nelson associates for the national serials data program', tags=[33]),\n",
       " TaggedDocument(words='these items had been placed in storage with the central services division', tags=[34]),\n",
       " TaggedDocument(words='office and personal files from the late josephine s pulsifer who in addition to being a staff member of the marc development office and the automation planning and liaison office had worked for the washington state library at the time of the marc pilot project and later for a consultant in the washington area', tags=[35]),\n",
       " TaggedDocument(words='most of the items consist of background material for the first edition of a marc history and the publications collected for a bibliography of works written by henriette d avram', tags=[36]),\n",
       " TaggedDocument(words='miscellaneous reports and documents that have been obtained sporadically to add to the archives', tags=[37]),\n",
       " TaggedDocument(words='includes several photographs', tags=[38]),\n",
       " TaggedDocument(words='copies of some items from contract files of the council on library resources washington dc dealing with projects sponsored by the council that predated and were the precursors of the marc pilot project', tags=[39]),\n",
       " TaggedDocument(words='although the library of congress had been involved with these projects the studies were performed by an outside consultant', tags=[40]),\n",
       " TaggedDocument(words='also included some of the items dealing with the marc pilot project', tags=[41]),\n",
       " TaggedDocument(words='the records of the henriette d avram marc development collection were processed in  by lenore s maruyama maruyama associates inc arlington va the finding aid was revised in  and ', tags=[42]),\n",
       " TaggedDocument(words='this collection is arranged in fourteen series arranged in three subseries automated systems officemarc development office office of the information systems specialist and information systems office', tags=[43])]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(sentenceSplit(docs_numberless[0]))]\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 17:47:07,550 - Lbl2Vec - INFO - Train document and word embeddings\n",
      "2023-03-17 17:47:07,550 - Lbl2Vec - INFO - Train document and word embeddings\n",
      "2023-03-17 17:47:07,550 - Lbl2Vec - INFO - Train document and word embeddings\n",
      "2023-03-17 17:47:07,610 - Lbl2Vec - INFO - Train label embeddings\n",
      "2023-03-17 17:47:07,610 - Lbl2Vec - INFO - Train label embeddings\n",
      "2023-03-17 17:47:07,610 - Lbl2Vec - INFO - Train label embeddings\n",
      "2023-03-17 17:47:07,612 - Lbl2Vec - WARNING - The following keywords from the 'keywords_list' are unknown to the Doc2Vec model and therefore not used to train the model: access accession accretion accrual acquisition administrative agency agent appraisal audit authentication authority backlog backup batch best practice bind bit born digital bulk bureaucracy business process byte call number caption capture catalog central certificate character check checksum circulation class classified closed code collection collective common community compatibility condition conservation contact container content context continuity contract contact convention conversion copy copyright creator crosswalk crowdsource cull curation curator custodial custody DACS data data architecture database database management system date deaccession deacidification declassification decoding decolonize deed deed of gift degradation deposit derivative work description descriptive bibliography descriptive cataloging descriptive element descriptive metadata descriptive record descriptive standard descriptive unit descriptor destruction destruction schedule digest digital digital forensics digitize disaster disposal document donation donor donor agreement duplicate email embedded archivist encapsulation enclosure Encoded Archival Context Encoded Archival Description encryption enduring value ephemera evaluation evidence evidential value exhibit extensible processing extent fair use file finding aid fixity folder fonds format function functional analysis genealogy genre gift gift agreement guide hard copy hard drive hardware hidden collection hierarchical database hierarchical description historical value housekeeping record HTML human-readable identity management imaging increment index indexing informational value information system ingest institutional memory institutional repository instrument integrity intellectual control intermediate interoperability intrinsic value inventory item item-level cataloging iterative processing journal judgment jurisdiction justification key keyword knowledge management law legacy letter liability license life continuum life cycle list local record long-term value lossless machine-readable macroappraisal magnification mail manuscript map master material media media migration memory memory worker metadata multilevel description name natural negative network noncustodial normalization object off-site oral history organic collection organization organizational record original original order orphan work outreach oversize page papers patron performance permanence permission persistent personal physical control pixel postcustodial preservation preservation copy preventive conservation primary source primary use print procedure production profile program prototype provenance public publication public domain public history range read reappraisal record red rot reference registry regulation reintegration relational database relative humidity reliability remote repair reparative repository reproduction reprocessing request researcher resolution resource respect des fonds responsibility restitution restoration restriction retention retention schedule rights risk scale scan schema schedule schema scope and contents search secondary section sensitive separation series source splice stability standard statute subgroup subheading subject subscription subsection subtitle survey symbolic value system tag taxonomy technical access requirements note terminal text timeliness time stamp topic trademark transaction transcript transfer transmission trusted digital repository trustworthiness uniqueness unit of description unprocessed unqualified upload use analysis use copy user use restriction user interface user study validation value variant verification version virtual virtual reading room visual materials vocabulary volume web archives weeding withdrawal witness work working files workstation writ write\n",
      "2023-03-17 17:47:07,612 - Lbl2Vec - WARNING - The following keywords from the 'keywords_list' are unknown to the Doc2Vec model and therefore not used to train the model: access accession accretion accrual acquisition administrative agency agent appraisal audit authentication authority backlog backup batch best practice bind bit born digital bulk bureaucracy business process byte call number caption capture catalog central certificate character check checksum circulation class classified closed code collection collective common community compatibility condition conservation contact container content context continuity contract contact convention conversion copy copyright creator crosswalk crowdsource cull curation curator custodial custody DACS data data architecture database database management system date deaccession deacidification declassification decoding decolonize deed deed of gift degradation deposit derivative work description descriptive bibliography descriptive cataloging descriptive element descriptive metadata descriptive record descriptive standard descriptive unit descriptor destruction destruction schedule digest digital digital forensics digitize disaster disposal document donation donor donor agreement duplicate email embedded archivist encapsulation enclosure Encoded Archival Context Encoded Archival Description encryption enduring value ephemera evaluation evidence evidential value exhibit extensible processing extent fair use file finding aid fixity folder fonds format function functional analysis genealogy genre gift gift agreement guide hard copy hard drive hardware hidden collection hierarchical database hierarchical description historical value housekeeping record HTML human-readable identity management imaging increment index indexing informational value information system ingest institutional memory institutional repository instrument integrity intellectual control intermediate interoperability intrinsic value inventory item item-level cataloging iterative processing journal judgment jurisdiction justification key keyword knowledge management law legacy letter liability license life continuum life cycle list local record long-term value lossless machine-readable macroappraisal magnification mail manuscript map master material media media migration memory memory worker metadata multilevel description name natural negative network noncustodial normalization object off-site oral history organic collection organization organizational record original original order orphan work outreach oversize page papers patron performance permanence permission persistent personal physical control pixel postcustodial preservation preservation copy preventive conservation primary source primary use print procedure production profile program prototype provenance public publication public domain public history range read reappraisal record red rot reference registry regulation reintegration relational database relative humidity reliability remote repair reparative repository reproduction reprocessing request researcher resolution resource respect des fonds responsibility restitution restoration restriction retention retention schedule rights risk scale scan schema schedule schema scope and contents search secondary section sensitive separation series source splice stability standard statute subgroup subheading subject subscription subsection subtitle survey symbolic value system tag taxonomy technical access requirements note terminal text timeliness time stamp topic trademark transaction transcript transfer transmission trusted digital repository trustworthiness uniqueness unit of description unprocessed unqualified upload use analysis use copy user use restriction user interface user study validation value variant verification version virtual virtual reading room visual materials vocabulary volume web archives weeding withdrawal witness work working files workstation writ write\n",
      "2023-03-17 17:47:07,612 - Lbl2Vec - WARNING - The following keywords from the 'keywords_list' are unknown to the Doc2Vec model and therefore not used to train the model: access accession accretion accrual acquisition administrative agency agent appraisal audit authentication authority backlog backup batch best practice bind bit born digital bulk bureaucracy business process byte call number caption capture catalog central certificate character check checksum circulation class classified closed code collection collective common community compatibility condition conservation contact container content context continuity contract contact convention conversion copy copyright creator crosswalk crowdsource cull curation curator custodial custody DACS data data architecture database database management system date deaccession deacidification declassification decoding decolonize deed deed of gift degradation deposit derivative work description descriptive bibliography descriptive cataloging descriptive element descriptive metadata descriptive record descriptive standard descriptive unit descriptor destruction destruction schedule digest digital digital forensics digitize disaster disposal document donation donor donor agreement duplicate email embedded archivist encapsulation enclosure Encoded Archival Context Encoded Archival Description encryption enduring value ephemera evaluation evidence evidential value exhibit extensible processing extent fair use file finding aid fixity folder fonds format function functional analysis genealogy genre gift gift agreement guide hard copy hard drive hardware hidden collection hierarchical database hierarchical description historical value housekeeping record HTML human-readable identity management imaging increment index indexing informational value information system ingest institutional memory institutional repository instrument integrity intellectual control intermediate interoperability intrinsic value inventory item item-level cataloging iterative processing journal judgment jurisdiction justification key keyword knowledge management law legacy letter liability license life continuum life cycle list local record long-term value lossless machine-readable macroappraisal magnification mail manuscript map master material media media migration memory memory worker metadata multilevel description name natural negative network noncustodial normalization object off-site oral history organic collection organization organizational record original original order orphan work outreach oversize page papers patron performance permanence permission persistent personal physical control pixel postcustodial preservation preservation copy preventive conservation primary source primary use print procedure production profile program prototype provenance public publication public domain public history range read reappraisal record red rot reference registry regulation reintegration relational database relative humidity reliability remote repair reparative repository reproduction reprocessing request researcher resolution resource respect des fonds responsibility restitution restoration restriction retention retention schedule rights risk scale scan schema schedule schema scope and contents search secondary section sensitive separation series source splice stability standard statute subgroup subheading subject subscription subsection subtitle survey symbolic value system tag taxonomy technical access requirements note terminal text timeliness time stamp topic trademark transaction transcript transfer transmission trusted digital repository trustworthiness uniqueness unit of description unprocessed unqualified upload use analysis use copy user use restriction user interface user study validation value variant verification version virtual virtual reading room visual materials vocabulary volume web archives weeding withdrawal witness work working files workstation writ write\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot compute mean with no input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7228/3255551653.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLbl2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeywords_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtermsList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidfTerms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordFrequencies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagged_documents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\lbl2vec\\lbl2vec.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;31m# get doc keys and similarity scores of documents that are similar to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;31m# the description keywords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m         self.labels[['doc_keys', 'doc_similarity_scores']] = self.labels['description_keywords'].apply(\n\u001b[0m\u001b[0;32m    234\u001b[0m             lambda row: self._get_similar_documents(\n\u001b[0;32m    235\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2vec_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_docs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_num_docs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimilarity_threshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilarity_threshold\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4356\u001b[0m         \"\"\"\n\u001b[1;32m-> 4357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 \u001b[1;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m                 \u001b[1;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m                 mapped = lib.map_infer(\n\u001b[0m\u001b[0;32m   1099\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\lbl2vec\\lbl2vec.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;31m# the description keywords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         self.labels[['doc_keys', 'doc_similarity_scores']] = self.labels['description_keywords'].apply(\n\u001b[1;32m--> 234\u001b[1;33m             lambda row: self._get_similar_documents(\n\u001b[0m\u001b[0;32m    235\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2vec_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_docs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_num_docs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimilarity_threshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilarity_threshold\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m                 min_num_docs=self.min_num_docs))\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\lbl2vec\\lbl2vec.py\u001b[0m in \u001b[0;36m_get_similar_documents\u001b[1;34m(self, doc2vec_model, keywords, num_docs, similarity_threshold, min_num_docs)\u001b[0m\n\u001b[0;32m    621\u001b[0m                 keyword_vectors = [doc2vec_model.wv[word]\n\u001b[0;32m    622\u001b[0m                                    for word in cleaned_keywords_list]\n\u001b[1;32m--> 623\u001b[1;33m                 similar_docs = doc2vec_model.dv.most_similar(\n\u001b[0m\u001b[0;32m    624\u001b[0m                     positive=keyword_vectors, topn=num_docs)\n\u001b[0;32m    625\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    839\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m         \u001b[1;31m# compute the weighted average of all keys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m         \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_mean_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre_normalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpost_normalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m         all_keys = [\n\u001b[0;32m    843\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_mean_vector\u001b[1;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[0;32m    494\u001b[0m         \"\"\"\n\u001b[0;32m    495\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cannot compute mean with no input\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m             \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot compute mean with no input"
     ]
    }
   ],
   "source": [
    "model = Lbl2Vec(keywords_list = [termsList, tfidfTerms(wordFrequencies,0,10)], tagged_documents=documents)\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceSplit(docs_numberless[0])\n",
    "\n",
    "def termCheck(terms, sentences):\n",
    "    contains = [0]*len(sentences)\n",
    "    for term in terms:\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if term in sentence:\n",
    "                contains[i]=1\n",
    "    \n",
    "    return pd.DataFrame(np.array([sentences,contains]).transpose(), columns = ['sentence','contains'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9318181818181818"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(termCheck(termsList, sentenceSplit(docs_numberless[0]))['contains']).astype(int).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan:\n",
    "\n",
    "1. Generate N per-FA keywords via TF-IDF for the paper within the whole corpus (adjust N as hyperparameter? start around 10)\n",
    "2. Use pre-existing keywords as archiving practice identifiers (try with all of them, or maybe just most common N if that has trouble training)\n",
    "3. Test Lbl2Vec using sentences as documents in this case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
